% !TeX spellcheck = en_US
% !TeX program = pdflatex

\documentclass{article}

\usepackage{amsmath}
\usepackage{url}
\usepackage{graphicx}
\usepackage{float}
\usepackage{amsthm}
\usepackage[a4paper, margin=1in]{geometry}
\usepackage{amssymb}
\usepackage{enumerate}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{problem}{Problem}
\newtheorem{example}{Example}[section]

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{corollary}{Corollary}[theorem]

\theoremstyle{remark}
\newtheorem*{remark}{Remark}

\renewcommand{\Re}{\operatorname{Re}}
\renewcommand{\Im}{\operatorname{Im}}
\newcommand{\reals}{\mathbb{R}}

\title{Report on Characteristic Functions}
\author{Fu Tianwen \and Yao Chaorui \and Zhao Feng}

\begin{document}
\maketitle

\section{Propose of Presentation}
In ESTR2002, it is quite common to deal with the sum of independent random variables, or approximate sums of random variables by assuming independence. However, it is pretty difficult and tedious to handle these kind of problems with convolution. With simple knowledge of complex numbers, \textbf{Characteristic Functions} can be very helpful by turning convolution into multiplication.\\
In the presentation, we give a brief introduction about the definition, background and properties related to characteristic functions. A brief proof of \textit{Central Limit Theorem} is also given. However, we did not give any proof for the properties nor the detailed and rigorous proof of CLT because we wanted to keep only the intuition rather than the tedious details of analysis.\\
To improve our presentation quality, we prepared a handout for our fellow classmates to read during the presentation, and put all the related mathematical statements on the slide. However, since the handout is posted too late, few classmates have them on hand during the presentation; statements on the slide also make the presentation going too fast compared with handwriting them on the whiteboard. Above would be most mistakes in the reflection of our presentation.
\section{Major Findings}
Below shows the most important findings, which are almost the same as the notes given in Piazza.
\subsection{Definition and Relation with Moments}
\begin{definition}
	Let $X$ be a random variable and denote by $F$ the cumulative distribution function of $X$ (or $f$ the probability density function). The characteristic function $\varphi=\varphi_X$ of $X$ (or of $F$, in which case we also write $\varphi_F$) is defined by \cite{cfms}
	$$\varphi_X(t):=E[e^{itX}]=\int_{-\infty}^\infty e^{itx}dF(x)=\int_{-\infty}^\infty e^{itx}f(x)dx, t\in\reals$$
\end{definition}
\noindent The $i$ mentioned above is the imaginary unit. If you have no knowledge of complex numbers, all you need to know is simply:
\begin{enumerate}[$\ast$]
	\item $i^2=-1$,
	\item $e^{i\theta} = \cos\theta + i\sin\theta$. (This formula comes from Taylor Series)
\end{enumerate}
\begin{definition}[Moment \cite{wiki:moment}]
	For probability density functions $f$ (or cumulative density function $F$), the moments are given by
	$$\mu_n' = E[X^n] = \int_{-\infty}^\infty x^ndF(x) = \int_{-\infty}^\infty x^nf(x)dx $$
\end{definition}
\begin{theorem}
	Suppose $X$ is a random variable for which the $n$th moment exists. Suppose $\varphi_X(t)$ is the characteristic function of $X$, and $\varphi_X(t)$ is $n$-times differentiable. Then the $n$th moment $$E[X^n]=i^{-n}\biggl[\frac{d^n}{dt^n}\varphi_X(t)\biggr]_{t=0}$$
\end{theorem}
\textbf{Conclusion.} From the theorem one may connect the moments (as in statistics) and the characteristic function (as in probability). It is easy to get the moments from the distribution just directly by definition; however, with characteristic functions, we may retrieve the distribution with all the statistics.
\subsection{Properties of Characteristic Function}
\begin{theorem}[Uniqueness Theorem \cite{uChicago}]
	Let $X$ be a real random variable with distribution function
	$F$ and characteristic function $\varphi$. Similarly, let $Y$ have distribution
	function $G$ and characteristic function $\psi$. If $\varphi(t) = \psi(t)$ for all $t\in\reals$
	then $F(x) = G(x)$ for all $x \in\reals$.
\end{theorem}
\begin{remark}
	From this we may easily conclude the distribution of a random variable if we can prove that its characteristic function is of the same form as a known distribution.
\end{remark}
\begin{theorem}[Inversion Formula \cite{pte}]
	If $\int_\reals |\varphi(t)| dt < \infty$ then $X$ has bounded continuous density
	$$
	f(x) = \frac1{2\pi} \int_\reals e^{-itx}\varphi(t)dt
	$$
\end{theorem}
\begin{remark}
	This gives us a way to calculate the distribution with characteristic functions.
\end{remark}
\begin{theorem}
	If $X$ and $Y$ are independent random variables then the characteristic function of their sum is\footnote{Properties \ref{thm:indept} and \ref{thm:linear} on come from Bisgaard and Zoltan's book \cite{cfms}.} 
	$$\varphi_{X+Y}(t) = \varphi_{X}\cdot\varphi_{Y}.$$
	\label{thm:indept}
\end{theorem}
\begin{remark}
	This gives us a much simpler way than convolution.
\end{remark} 
\begin{remark}
	If $X$ and $Y$ are random variables such that $\varphi_{X+Y} = \varphi_X\cdot\varphi_Y $, then in general we do not conclude $X$ and $Y$ are independent. (See page 13 in \cite{cfms}) This is called \textbf{subindependence}\cite{wiki:subindependence}.
\end{remark} 
\begin{corollary}
	The product of two characteristic functions is a characteristic function.
\end{corollary}
\begin{theorem}
	For any $a,b \in \reals$, $$\varphi_{aX+b}(t)=e^{ibt}\varphi_X(at).$$
	\label{thm:linear}
\end{theorem}
\begin{table}[ht]
	\caption{Characteristic Functions for Common Distributions\cite{kurser}}
	\centering
	\begin{tabular}{l l l}
		\hline\hline
		Distribution  & PMF/PDF & Characteristic Function\\
		\hline
		Constant $X\equiv a$  & - &  $\varphi_X(t) = e^{iat}.$\\
		Binomial $X\sim Binomial(m,p)$ & $p_X(n) = \binom{n}{m} p^n(1-p)^{m-n}$ &
		$\varphi_X(t) = (pe^{it} + (1-p))^m$\\
		Poisson $X\sim Poisson(\lambda)$ & $p_X(n) = \frac{\lambda ^n}{n!} e^{-\lambda}$ &
		$ \varphi_X(t)=e^{\lambda(e^{it}-1)} $\\
		Exponential $X \sim Exponential(\lambda)$ & $p_X(n) = \lambda e^{-\lambda x}$ &
		$\varphi_X(t)=\frac{\lambda}{\lambda-it}$ \\
		Normal $X\sim N(0,1)$ & $f_X(x) = \frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}$ &
		$\varphi_X(t)=e^{-\frac{t^2}{2}}$\\
		Normal $Y\sim N(\mu,\sigma ^2)$ & $f_Y(y) = \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(y-\mu)^2}{2\sigma ^2}}$ & $\varphi_Y(t)=e^{it\mu-\frac{\sigma^2 t^2}{2}}$
	\end{tabular}
	\label{tbl:charFunc}
\end{table}
\subsection{Examples and Applications}
\begin{example}
	Rain falls on your head at $\lambda$ drops per second on average. What is the distribution of rain drops on your head in two seconds?
\end{example}
\noindent\textbf{Solution.}
Let $X,Y$ be two independent $Poisson(\lambda)$ random variables. Let $Z=X+Y$.
Notice that characteristic functions for $X$ (and respectively $Y$) is $\varphi_X(t)=e^{\lambda(e^{it}-1)}$.
Therefore we have $\varphi_Z(t)=\varphi_{X+Y}(t)=(\varphi_X(t))^2=e^{2\lambda(e^{it}-1)}$. 
By uniqueness of characteristic functions we know that $Z\sim Poisson(2\lambda)$.
\begin{remark}
	By similar ideas, one can show that the sum of two independent poisson random variables has a possion distribution with an expectation of the sum of both expectations.
\end{remark}
\begin{example}
	$X_1\sim N(\mu_1,\sigma_1^2), X_2\sim N(\mu_2,\sigma_2^2).$ $X_1,X_2$ are independent. Distribution of $Y=X_1+X_2$?
\end{example}
\noindent\textbf{Solution.}
Similarly to last example $$\varphi_{Y}(t)=\varphi_{X_1+X_2}(t)=e^{it\mu_1-\frac{\sigma_1^2 t^2}{2}}\cdot e^{it\mu_2-\frac{\sigma_2^2 t^2}{2}}$$
Therefore we have $$\varphi_Y(t)=e^{it(\mu_1+\mu_2)-\frac{(\sigma_1^2+\sigma_2^2)t^2}{2}}$$
which implies that $Y\sim N(\mu_1+\mu_2,\sigma_1^2+\sigma_2^2)$

\subsection{Central Limit Theorem}
First we prove a useful lemma\footnote{The proofs for lemma \ref{lem:clt} and theorem \ref{thm:clt} are not rigorous and serves only for intuitively demonstrating the usage of Characteristic Functions. See the references\cite{waterloo,nus} for detailed proofs.}:
\begin{lemma}
	For any random variable $X$ with $E[X]=0,Var[X]=1$, we have $\varphi_X(t)=1-\frac12t^2+o(t^2)$.
	\label{lem:clt}
\end{lemma}
\begin{proof}
	Directly from the definition and Taylor Series we have
	$$\varphi_X(t)=E[e^{itX}]=E[1+itX+\frac12i^2t^2X^2+o((tX)^2)]$$
	By linearity of expectation,
	$$\varphi_X(t)=E[1]+itE[X]-\frac12t^2E[X^2]+o(t^2)$$
	Also
	$$E[X^2]=Var[X]-(E[X])^2=1$$
	Therefore $$\varphi_X(t)=1-\frac12t^2+o(t^2)$$
\end{proof}
\begin{theorem}[Central Limit Theorem\cite{waterloo}]
	If $X_i$ are independent identically distributed random variables with $E[X_i] = \mu, Var[X_i] = \sigma^2$ , then $S_n^* = \frac1{\sigma\sqrt n}\sum_{i=1}^n(X_i-\mu)$ converges weakly to $N(0, 1)$.
	\label{thm:clt}
\end{theorem}
\begin{remark}
	Note that theorem \ref{thm:clt} is equivalent to the one given in class.
\end{remark}
\begin{proof}
	By Lemma \ref{lem:clt} we denote the characteristic function of $\frac{X_i-\mu}{\sigma}$ by $\varphi(t)$, then  $\varphi(t)=1-\frac12t^2+o(t^2)$. Therefore the characteristic function of $S_n^*$ is 
	$$\varphi^n(t/\sqrt{n})=[1-\frac{t^2}{2n}+o(t^2/n)]^n$$
	Take $n\to\infty$ and we get the characteristic function of $\lim\limits_{n\to\infty}S^*_n$
	$$\varPhi(t)=\lim\limits_{n\to\infty}\varphi^n(t/\sqrt{n})
	=\lim\limits_{n\to\infty}[1-\frac{t^2}{2n}]^n=e^{-\frac{t^2}2}$$
	Therefore $\lim\limits_{n\to\infty}S_n^*$ converges to $N(0,1)$.
\end{proof}
\medskip

\bibliographystyle{ieeetr}
\bibliography{bibliography}
\end{document}